#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è½¯è‘—ç”³è¯·ææ–™è´¨é‡ç›‘æ§å’Œæ£€æµ‹å·¥å…·
åŠŸèƒ½ï¼šå…¨æ–¹ä½ç›‘æ§ç”Ÿæˆè¿‡ç¨‹çš„è´¨é‡ï¼Œæä¾›å®æ—¶åé¦ˆå’Œæ™ºèƒ½å»ºè®®

ç›‘æ§ç»´åº¦ï¼š
1. ç”Ÿæˆè¿›åº¦è·Ÿè¸ª
2. ä»£ç è´¨é‡æ£€æµ‹
3. æ–‡æ¡£å®Œæ•´æ€§éªŒè¯
4. ç”³è¯·æˆåŠŸç‡é¢„æµ‹
5. æ€§èƒ½æŒ‡æ ‡åˆ†æ
"""

import sys
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# é¢œè‰²è¾“å‡ºç±»
class Colors:
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    PURPLE = '\033[0;35m'
    CYAN = '\033[0;36m'
    NC = '\033[0m'  # No Color

def print_success(message: str):
    print(f"{Colors.GREEN}âœ“ {message}{Colors.NC}")

def print_info(message: str):
    print(f"{Colors.BLUE}â„¹ {message}{Colors.NC}")

def print_warning(message: str):
    print(f"{Colors.YELLOW}âš  {message}{Colors.NC}")

def print_error(message: str):
    print(f"{Colors.RED}âœ— {message}{Colors.NC}")

def print_header(message: str):
    print(f"{Colors.PURPLE}{'=' * 80}{Colors.NC}")
    print(f"{Colors.PURPLE}{message.center(80)}{Colors.NC}")
    print(f"{Colors.PURPLE}{'=' * 80}{Colors.NC}")

class QualityMonitor:
    """è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.project_root = Path.cwd()
        self.config_path = self.project_root / "ai-copyright-config.json"
        self.monitoring_results = {}
        
    def load_config(self) -> Optional[dict]:
        """åŠ è½½é¡¹ç›®é…ç½®"""
        if not self.config_path.exists():
            return None
        
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return None
    
    def check_generation_progress(self) -> Dict[str, any]:
        """æ£€æŸ¥ç”Ÿæˆè¿›åº¦"""
        progress = {
            'requirements_doc': False,
            'framework_design': False,
            'page_list': False,
            'frontend_code': False,
            'backend_code': False,
            'database_code': False,
            'user_manual': False,
            'registration_form': False,
            'merged_frontend': False,
            'merged_backend': False,
            'merged_database': False
        }
        
        file_mappings = {
            'requirements_doc': 'requires_docs/éœ€æ±‚æ–‡æ¡£.md',
            'framework_design': 'process_docs/*æ¡†æ¶è®¾è®¡æ–‡æ¡£.md',
            'page_list': 'process_docs/é¡µé¢æ¸…å•.md',
            'frontend_code': 'output_sourcecode/front/*.html',
            'backend_code': 'output_sourcecode/backend/*',
            'database_code': 'output_sourcecode/db/*.sql',
            'user_manual': 'output_docs/*ç”¨æˆ·æ‰‹å†Œ.md',
            'registration_form': 'output_docs/*è½¯ä»¶è‘—ä½œæƒç™»è®°ä¿¡æ¯è¡¨.md',
            'merged_frontend': 'output_docs/å‰ç«¯æºä»£ç .txt',
            'merged_backend': 'output_docs/åç«¯æºä»£ç .txt',
            'merged_database': 'output_docs/æ•°æ®åº“æºä»£ç .txt'
        }
        
        for key, pattern in file_mappings.items():
            if '*' in pattern:
                # ä½¿ç”¨globåŒ¹é…
                matches = list(self.project_root.glob(pattern))
                progress[key] = len(matches) > 0
            else:
                # ç›´æ¥æ£€æŸ¥æ–‡ä»¶
                file_path = self.project_root / pattern
                progress[key] = file_path.exists()
        
        # è®¡ç®—å®Œæˆåº¦
        completed_stages = sum(progress.values())
        total_stages = len(progress)
        completion_rate = (completed_stages / total_stages) * 100
        
        return {
            'progress': progress,
            'completed_stages': completed_stages,
            'total_stages': total_stages,
            'completion_rate': completion_rate
        }
    
    def analyze_code_quality(self) -> Dict[str, any]:
        """åˆ†æä»£ç è´¨é‡"""
        quality_metrics = {
            'frontend': self.analyze_frontend_quality(),
            'backend': self.analyze_backend_quality(),
            'database': self.analyze_database_quality()
        }
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        quality_scores = [metrics['quality_score'] for metrics in quality_metrics.values() if metrics['quality_score'] > 0]
        overall_score = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        return {
            'components': quality_metrics,
            'overall_score': overall_score,
            'quality_level': self.get_quality_level(overall_score)
        }
    
    def analyze_frontend_quality(self) -> Dict[str, any]:
        """åˆ†æå‰ç«¯ä»£ç è´¨é‡"""
        frontend_dir = self.project_root / "output_sourcecode" / "front"
        
        if not frontend_dir.exists():
            return {'exists': False, 'quality_score': 0}
        
        html_files = list(frontend_dir.glob("*.html"))
        if not html_files:
            return {'exists': False, 'quality_score': 0}
        
        metrics = {
            'file_count': len(html_files),
            'total_size': 0,
            'avg_size': 0,
            'has_css': 0,
            'has_js': 0,
            'has_responsive': 0,
            'has_navigation': 0,
            'html5_compliant': 0,
            'quality_score': 0
        }
        
        for html_file in html_files:
            try:
                with open(html_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                file_size = len(content)
                metrics['total_size'] += file_size
                
                # æ£€æŸ¥å„ç§è´¨é‡æŒ‡æ ‡
                if '<style>' in content or 'class=' in content:
                    metrics['has_css'] += 1
                
                if '<script>' in content or 'function' in content:
                    metrics['has_js'] += 1
                
                if 'responsive' in content.lower() or '@media' in content:
                    metrics['has_responsive'] += 1
                
                if '<nav>' in content or 'navigation' in content.lower():
                    metrics['has_navigation'] += 1
                
                if '<!DOCTYPE html>' in content:
                    metrics['html5_compliant'] += 1
                    
            except:
                continue
        
        if metrics['file_count'] > 0:
            metrics['avg_size'] = metrics['total_size'] / metrics['file_count']
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            score = 0
            score += min(metrics['file_count'] * 10, 50)  # æ–‡ä»¶æ•°é‡ (æœ€å¤š50åˆ†)
            score += (metrics['has_css'] / metrics['file_count']) * 15  # CSSä½¿ç”¨ç‡ (15åˆ†)
            score += (metrics['has_js'] / metrics['file_count']) * 15  # JSä½¿ç”¨ç‡ (15åˆ†)
            score += (metrics['has_responsive'] / metrics['file_count']) * 10  # å“åº”å¼ (10åˆ†)
            score += (metrics['has_navigation'] / metrics['file_count']) * 5  # å¯¼èˆª (5åˆ†)
            score += (metrics['html5_compliant'] / metrics['file_count']) * 5  # HTML5 (5åˆ†)
            
            metrics['quality_score'] = min(score, 100)
        
        metrics['exists'] = True
        return metrics
    
    def analyze_backend_quality(self) -> Dict[str, any]:
        """åˆ†æåç«¯ä»£ç è´¨é‡"""
        backend_dir = self.project_root / "output_sourcecode" / "backend"
        
        if not backend_dir.exists():
            return {'exists': False, 'quality_score': 0}
        
        # æ”¶é›†æºä»£ç æ–‡ä»¶
        source_extensions = ['.java', '.py', '.js', '.php', '.cs', '.go', '.rb']
        source_files = []
        for ext in source_extensions:
            source_files.extend(backend_dir.rglob(f"*{ext}"))
        
        if not source_files:
            return {'exists': False, 'quality_score': 0}
        
        metrics = {
            'file_count': len(source_files),
            'total_size': 0,
            'avg_size': 0,
            'has_functions': 0,
            'has_classes': 0,
            'has_comments': 0,
            'has_error_handling': 0,
            'complexity_score': 0,
            'quality_score': 0
        }
        
        for source_file in source_files:
            try:
                with open(source_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                file_size = len(content)
                metrics['total_size'] += file_size
                
                # æ£€æŸ¥ä»£ç ç‰¹å¾
                if re.search(r'function\s+\w+|def\s+\w+|public\s+\w+\s+\w+\s*\(', content):
                    metrics['has_functions'] += 1
                
                if re.search(r'class\s+\w+|public\s+class\s+\w+', content):
                    metrics['has_classes'] += 1
                
                if '//' in content or '/*' in content or '#' in content or '"""' in content:
                    metrics['has_comments'] += 1
                
                if re.search(r'try\s*{|except:|catch\s*\(|error|exception', content, re.IGNORECASE):
                    metrics['has_error_handling'] += 1
                
                # è®¡ç®—å¤æ‚åº¦åˆ†æ•°ï¼ˆåŸºäºå…³é”®è¯å¯†åº¦ï¼‰
                complexity_keywords = ['if', 'for', 'while', 'switch', 'case', 'else', 'elif']
                complexity_count = sum(content.lower().count(keyword) for keyword in complexity_keywords)
                metrics['complexity_score'] += complexity_count
                
            except:
                continue
        
        if metrics['file_count'] > 0:
            metrics['avg_size'] = metrics['total_size'] / metrics['file_count']
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            score = 0
            score += min(metrics['file_count'] * 8, 40)  # æ–‡ä»¶æ•°é‡ (æœ€å¤š40åˆ†)
            score += (metrics['has_functions'] / metrics['file_count']) * 20  # å‡½æ•°å®šä¹‰ (20åˆ†)
            score += (metrics['has_classes'] / metrics['file_count']) * 15  # ç±»å®šä¹‰ (15åˆ†)
            score += (metrics['has_comments'] / metrics['file_count']) * 10  # æ³¨é‡Š (10åˆ†)
            score += (metrics['has_error_handling'] / metrics['file_count']) * 10  # é”™è¯¯å¤„ç† (10åˆ†)
            score += min(metrics['complexity_score'] / metrics['file_count'], 5)  # å¤æ‚åº¦ (5åˆ†)
            
            metrics['quality_score'] = min(score, 100)
        
        metrics['exists'] = True
        return metrics
    
    def analyze_database_quality(self) -> Dict[str, any]:
        """åˆ†ææ•°æ®åº“è´¨é‡"""
        db_dir = self.project_root / "output_sourcecode" / "db"
        
        if not db_dir.exists():
            return {'exists': False, 'quality_score': 0}
        
        sql_files = list(db_dir.glob("*.sql"))
        if not sql_files:
            return {'exists': False, 'quality_score': 0}
        
        metrics = {
            'file_count': len(sql_files),
            'total_size': 0,
            'create_table_count': 0,
            'index_count': 0,
            'constraint_count': 0,
            'procedure_count': 0,
            'view_count': 0,
            'quality_score': 0
        }
        
        for sql_file in sql_files:
            try:
                with open(sql_file, 'r', encoding='utf-8') as f:
                    content = f.read().upper()
                
                metrics['total_size'] += len(content)
                metrics['create_table_count'] += content.count('CREATE TABLE')
                metrics['index_count'] += content.count('CREATE INDEX')
                metrics['constraint_count'] += content.count('CONSTRAINT') + content.count('PRIMARY KEY') + content.count('FOREIGN KEY')
                metrics['procedure_count'] += content.count('CREATE PROCEDURE') + content.count('CREATE FUNCTION')
                metrics['view_count'] += content.count('CREATE VIEW')
                
            except:
                continue
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        score = 0
        score += min(metrics['create_table_count'] * 10, 40)  # è¡¨æ•°é‡ (æœ€å¤š40åˆ†)
        score += min(metrics['index_count'] * 5, 15)  # ç´¢å¼• (æœ€å¤š15åˆ†)
        score += min(metrics['constraint_count'] * 3, 15)  # çº¦æŸ (æœ€å¤š15åˆ†)
        score += min(metrics['procedure_count'] * 10, 20)  # å­˜å‚¨è¿‡ç¨‹ (æœ€å¤š20åˆ†)
        score += min(metrics['view_count'] * 5, 10)  # è§†å›¾ (æœ€å¤š10åˆ†)
        
        metrics['quality_score'] = min(score, 100)
        metrics['exists'] = True
        return metrics
    
    def get_quality_level(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è·å–è´¨é‡ç­‰çº§"""
        if score >= 90:
            return "ä¼˜ç§€"
        elif score >= 75:
            return "è‰¯å¥½"
        elif score >= 60:
            return "åŠæ ¼"
        elif score >= 40:
            return "è¾ƒå·®"
        else:
            return "å¾ˆå·®"
    
    def predict_approval_probability(self, progress_data: Dict, quality_data: Dict) -> Dict[str, any]:
        """é¢„æµ‹ç”³è¯·æˆåŠŸç‡"""
        # åŸºäºè¿›åº¦å’Œè´¨é‡æ•°æ®é¢„æµ‹æˆåŠŸç‡
        progress_weight = 0.4
        quality_weight = 0.6
        
        progress_score = progress_data['completion_rate']
        quality_score = quality_data['overall_score']
        
        # è®¡ç®—åŠ æƒåˆ†æ•°
        weighted_score = (progress_score * progress_weight + quality_score * quality_weight)
        
        # è½¬æ¢ä¸ºæˆåŠŸç‡
        if weighted_score >= 90:
            probability = 0.95
            level = "å¾ˆé«˜"
        elif weighted_score >= 80:
            probability = 0.85
            level = "é«˜"
        elif weighted_score >= 70:
            probability = 0.70
            level = "ä¸­ç­‰"
        elif weighted_score >= 60:
            probability = 0.55
            level = "è¾ƒä½"
        else:
            probability = 0.30
            level = "ä½"
        
        return {
            'probability': probability,
            'percentage': probability * 100,
            'level': level,
            'weighted_score': weighted_score,
            'factors': {
                'progress_contribution': progress_score * progress_weight,
                'quality_contribution': quality_score * quality_weight
            }
        }
    
    def generate_recommendations(self, progress_data: Dict, quality_data: Dict, prediction: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºè¿›åº¦çš„å»ºè®®
        progress = progress_data['progress']
        if not progress['requirements_doc']:
            recommendations.append("ğŸ”´ ç´§æ€¥ï¼šåˆ›å»ºéœ€æ±‚æ–‡æ¡£æ˜¯æ‰€æœ‰åç»­å·¥ä½œçš„åŸºç¡€")
        elif not progress['framework_design']:
            recommendations.append("ğŸ”´ é‡è¦ï¼šç”Ÿæˆæ¡†æ¶è®¾è®¡æ–‡æ¡£ä»¥æŒ‡å¯¼åç»­å¼€å‘")
        elif not progress['frontend_code'] and not progress['backend_code']:
            recommendations.append("ğŸ”´ å…³é”®ï¼šå¼€å§‹ç”Ÿæˆå‰ç«¯å’Œåç«¯ä»£ç ")
        elif not progress['merged_frontend'] or not progress['merged_backend']:
            recommendations.append("ğŸŸ¡ å»ºè®®ï¼šæ‰§è¡Œä»£ç åˆå¹¶ä»¥ç”Ÿæˆç”³è¯·ææ–™")
        
        # åŸºäºè´¨é‡çš„å»ºè®®
        quality_components = quality_data['components']
        
        if quality_components['frontend']['exists'] and quality_components['frontend']['quality_score'] < 60:
            recommendations.append("ğŸ”§ å‰ç«¯ä»£ç è´¨é‡æœ‰å¾…æå‡ï¼šå¢åŠ CSSæ ·å¼ã€JavaScriptäº¤äº’å’Œå“åº”å¼è®¾è®¡")
        
        if quality_components['backend']['exists'] and quality_components['backend']['quality_score'] < 60:
            recommendations.append("ğŸ”§ åç«¯ä»£ç è´¨é‡æœ‰å¾…æå‡ï¼šå¢åŠ å‡½æ•°æ¨¡å—åŒ–ã€ç±»è®¾è®¡å’Œé”™è¯¯å¤„ç†")
        
        if quality_components['database']['exists'] and quality_components['database']['quality_score'] < 60:
            recommendations.append("ğŸ”§ æ•°æ®åº“è®¾è®¡æœ‰å¾…å®Œå–„ï¼šå¢åŠ è¡¨ç»“æ„ã€ç´¢å¼•å’Œçº¦æŸè®¾è®¡")
        
        # åŸºäºæˆåŠŸç‡é¢„æµ‹çš„å»ºè®®
        if prediction['probability'] < 0.7:
            recommendations.append("âš ï¸ å½“å‰ç”³è¯·æˆåŠŸç‡åä½ï¼Œå»ºè®®å…¨é¢æå‡æ–‡æ¡£è´¨é‡åå†æäº¤")
        elif prediction['probability'] < 0.85:
            recommendations.append("ğŸ’¡ ç”³è¯·æˆåŠŸç‡ä¸­ç­‰ï¼Œå¯è€ƒè™‘ä¼˜åŒ–éƒ¨åˆ†è–„å¼±ç¯èŠ‚")
        
        return recommendations
    
    def run_monitoring(self) -> Dict[str, any]:
        """æ‰§è¡Œå®Œæ•´ç›‘æ§"""
        print_header("è½¯è‘—ç”³è¯·ææ–™è´¨é‡ç›‘æ§")
        
        config = self.load_config()
        project_name = config.get('title', 'æœªçŸ¥é¡¹ç›®') if config else 'æœªçŸ¥é¡¹ç›®'
        
        print_info(f"é¡¹ç›®: {project_name}")
        print_info(f"ç›‘æ§æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # æ£€æŸ¥ç”Ÿæˆè¿›åº¦
        print_info("æ£€æŸ¥ç”Ÿæˆè¿›åº¦...")
        progress_data = self.check_generation_progress()
        
        # åˆ†æä»£ç è´¨é‡
        print_info("åˆ†æä»£ç è´¨é‡...")
        quality_data = self.analyze_code_quality()
        
        # é¢„æµ‹ç”³è¯·æˆåŠŸç‡
        print_info("è¯„ä¼°ç”³è¯·æˆåŠŸç‡...")
        prediction = self.predict_approval_probability(progress_data, quality_data)
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        recommendations = self.generate_recommendations(progress_data, quality_data, prediction)
        
        return {
            'project_name': project_name,
            'timestamp': datetime.now().isoformat(),
            'progress': progress_data,
            'quality': quality_data,
            'prediction': prediction,
            'recommendations': recommendations
        }

def generate_monitoring_report(monitoring_result: Dict) -> str:
    """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    project_name = monitoring_result['project_name']
    progress = monitoring_result['progress']
    quality = monitoring_result['quality']
    prediction = monitoring_result['prediction']
    recommendations = monitoring_result['recommendations']
    
    report = f"""
{'-' * 80}
è½¯è‘—ç”³è¯·ææ–™è´¨é‡ç›‘æ§æŠ¥å‘Š
{'-' * 80}

é¡¹ç›®åç§°: {project_name}
ç›‘æ§æ—¶é—´: {current_time}

{'-' * 80}
ç”Ÿæˆè¿›åº¦æ¦‚è§ˆ
{'-' * 80}

æ€»ä½“å®Œæˆåº¦: {progress['completion_rate']:.1f}% ({progress['completed_stages']}/{progress['total_stages']})

è¿›åº¦è¯¦æƒ…:
"""
    
    stage_names = {
        'requirements_doc': 'éœ€æ±‚æ–‡æ¡£',
        'framework_design': 'æ¡†æ¶è®¾è®¡',
        'page_list': 'é¡µé¢æ¸…å•',
        'frontend_code': 'å‰ç«¯ä»£ç ',
        'backend_code': 'åç«¯ä»£ç ',
        'database_code': 'æ•°æ®åº“ä»£ç ',
        'user_manual': 'ç”¨æˆ·æ‰‹å†Œ',
        'registration_form': 'ç™»è®°ä¿¡æ¯è¡¨',
        'merged_frontend': 'å‰ç«¯åˆå¹¶æ–‡æ¡£',
        'merged_backend': 'åç«¯åˆå¹¶æ–‡æ¡£',
        'merged_database': 'æ•°æ®åº“åˆå¹¶æ–‡æ¡£'
    }
    
    for key, completed in progress['progress'].items():
        status = "âœ“" if completed else "âœ—"
        name = stage_names.get(key, key)
        report += f"  {status} {name}\n"
    
    report += f"""
{'-' * 80}
ä»£ç è´¨é‡åˆ†æ
{'-' * 80}

æ€»ä½“è´¨é‡åˆ†æ•°: {quality['overall_score']:.1f}/100 ({quality['quality_level']})

ç»„ä»¶è´¨é‡è¯¦æƒ…:
"""
    
    # å‰ç«¯è´¨é‡
    frontend = quality['components']['frontend']
    if frontend['exists']:
        report += f"  âœ“ å‰ç«¯ä»£ç : {frontend['quality_score']:.1f}/100\n"
        report += f"    - é¡µé¢æ–‡ä»¶æ•°: {frontend['file_count']}\n"
        report += f"    - å¹³å‡æ–‡ä»¶å¤§å°: {frontend['avg_size']:.0f} å­—ç¬¦\n"
        report += f"    - CSSä½¿ç”¨ç‡: {frontend['has_css']}/{frontend['file_count']}\n"
        report += f"    - JavaScriptä½¿ç”¨ç‡: {frontend['has_js']}/{frontend['file_count']}\n"
    else:
        report += "  âœ— å‰ç«¯ä»£ç : æœªç”Ÿæˆ\n"
    
    # åç«¯è´¨é‡
    backend = quality['components']['backend']
    if backend['exists']:
        report += f"  âœ“ åç«¯ä»£ç : {backend['quality_score']:.1f}/100\n"
        report += f"    - æºæ–‡ä»¶æ•°: {backend['file_count']}\n"
        report += f"    - å‡½æ•°å®šä¹‰: {backend['has_functions']}/{backend['file_count']}\n"
        report += f"    - ç±»å®šä¹‰: {backend['has_classes']}/{backend['file_count']}\n"
        report += f"    - é”™è¯¯å¤„ç†: {backend['has_error_handling']}/{backend['file_count']}\n"
    else:
        report += "  âœ— åç«¯ä»£ç : æœªç”Ÿæˆ\n"
    
    # æ•°æ®åº“è´¨é‡
    database = quality['components']['database']
    if database['exists']:
        report += f"  âœ“ æ•°æ®åº“è®¾è®¡: {database['quality_score']:.1f}/100\n"
        report += f"    - æ•°æ®è¡¨æ•°: {database['create_table_count']}\n"
        report += f"    - ç´¢å¼•æ•°: {database['index_count']}\n"
        report += f"    - çº¦æŸæ•°: {database['constraint_count']}\n"
        report += f"    - å­˜å‚¨è¿‡ç¨‹æ•°: {database['procedure_count']}\n"
    else:
        report += "  âœ— æ•°æ®åº“è®¾è®¡: æœªç”Ÿæˆ\n"
    
    report += f"""
{'-' * 80}
ç”³è¯·æˆåŠŸç‡é¢„æµ‹
{'-' * 80}

é¢„æµ‹æˆåŠŸç‡: {prediction['percentage']:.1f}% ({prediction['level']})
åŠ æƒè¯„åˆ†: {prediction['weighted_score']:.1f}/100

è¯„åˆ†æ„æˆ:
  - è¿›åº¦è´¡çŒ®: {prediction['factors']['progress_contribution']:.1f}åˆ† (æƒé‡40%)
  - è´¨é‡è´¡çŒ®: {prediction['factors']['quality_contribution']:.1f}åˆ† (æƒé‡60%)

"""
    
    # æ·»åŠ æ”¹è¿›å»ºè®®
    if recommendations:
        report += f"{'-' * 80}\næ”¹è¿›å»ºè®®\n{'-' * 80}\n\n"
        for i, recommendation in enumerate(recommendations, 1):
            report += f"{i}. {recommendation}\n"
    
    # æ·»åŠ è¡ŒåŠ¨è®¡åˆ’
    report += f"""
{'-' * 80}
ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’
{'-' * 80}

"""
    
    if progress['completion_rate'] < 50:
        report += "ğŸ”´ å½“å‰å¤„äºåˆæœŸé˜¶æ®µï¼Œé‡ç‚¹ä»»åŠ¡:\n"
        report += "  1. å®Œå–„éœ€æ±‚æ–‡æ¡£å†…å®¹\n"
        report += "  2. ç”Ÿæˆæ¡†æ¶è®¾è®¡æ–‡æ¡£\n" 
        report += "  3. å¼€å§‹ä»£ç ç”Ÿæˆå·¥ä½œ\n"
    elif progress['completion_rate'] < 80:
        report += "ğŸŸ¡ å½“å‰å¤„äºå¼€å‘é˜¶æ®µï¼Œé‡ç‚¹ä»»åŠ¡:\n"
        report += "  1. å®Œæˆæ‰€æœ‰ä»£ç ç”Ÿæˆ\n"
        report += "  2. æå‡ä»£ç è´¨é‡å’Œå¤æ‚åº¦\n"
        report += "  3. å‡†å¤‡ç”³è¯·ææ–™åˆå¹¶\n"
    else:
        report += "ğŸŸ¢ å½“å‰å¤„äºæ”¶å°¾é˜¶æ®µï¼Œé‡ç‚¹ä»»åŠ¡:\n"
        report += "  1. æ‰§è¡Œä»£ç åˆå¹¶è„šæœ¬\n"
        report += "  2. ç”Ÿæˆç”¨æˆ·æ‰‹å†Œå’Œç™»è®°è¡¨\n"
        report += "  3. æœ€ç»ˆè´¨é‡æ£€æŸ¥å’Œæäº¤\n"
    
    if quality['overall_score'] < 60:
        report += "\nğŸ”§ è´¨é‡æå‡é‡ç‚¹:\n"
        report += "  1. å¢åŠ ä»£ç çš„åŠŸèƒ½å¤æ‚åº¦\n"
        report += "  2. å®Œå–„é”™è¯¯å¤„ç†å’Œå¼‚å¸¸ç®¡ç†\n"
        report += "  3. å¢å¼ºç”¨æˆ·ç•Œé¢å’Œäº¤äº’è®¾è®¡\n"
        report += "  4. ä¼˜åŒ–æ•°æ®åº“è®¾è®¡å’Œçº¦æŸ\n"
    
    report += f"\n{'-' * 80}\næŠ¥å‘Šç”Ÿæˆæ—¶é—´: {current_time}\n{'-' * 80}\n"
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help']:
        print("è½¯è‘—ç”³è¯·ææ–™è´¨é‡ç›‘æ§å·¥å…·")
        print("\nç”¨æ³•:")
        print("  python3 quality_monitor.py")
        print("\nåŠŸèƒ½:")
        print("  - ç›‘æ§ç”Ÿæˆè¿›åº¦å’Œè´¨é‡")
        print("  - åˆ†æä»£ç å¤æ‚åº¦å’Œä¸“ä¸šæ€§")
        print("  - é¢„æµ‹ç”³è¯·æˆåŠŸç‡")
        print("  - æä¾›æ”¹è¿›å»ºè®®å’Œè¡ŒåŠ¨è®¡åˆ’")
        print("\nè¾“å‡º:")
        print("  - ç»ˆç«¯æ˜¾ç¤ºç›‘æ§ç»“æœ")
        print("  - ç”Ÿæˆè¯¦ç»†çš„è´¨é‡ç›‘æ§æŠ¥å‘Š")
        return
    
    # æ‰§è¡Œç›‘æ§
    monitor = QualityMonitor()
    result = monitor.run_monitoring()
    
    # æ˜¾ç¤ºå…³é”®ç»“æœ
    print_header("ç›‘æ§ç»“æœæ¦‚è§ˆ")
    
    progress = result['progress']
    quality = result['quality']
    prediction = result['prediction']
    
    print_info(f"ç”Ÿæˆè¿›åº¦: {progress['completion_rate']:.1f}% ({progress['completed_stages']}/{progress['total_stages']})")
    print_info(f"ä»£ç è´¨é‡: {quality['overall_score']:.1f}/100 ({quality['quality_level']})")
    
    if prediction['probability'] >= 0.8:
        print_success(f"ç”³è¯·æˆåŠŸç‡: {prediction['percentage']:.1f}% ({prediction['level']})")
    elif prediction['probability'] >= 0.6:
        print_warning(f"ç”³è¯·æˆåŠŸç‡: {prediction['percentage']:.1f}% ({prediction['level']})")
    else:
        print_error(f"ç”³è¯·æˆåŠŸç‡: {prediction['percentage']:.1f}% ({prediction['level']})")
    
    # æ˜¾ç¤ºå…³é”®å»ºè®®
    if result['recommendations']:
        print()
        print_header("å…³é”®å»ºè®®")
        for i, recommendation in enumerate(result['recommendations'][:5], 1):  # æ˜¾ç¤ºå‰5æ¡
            print_info(f"{i}. {recommendation}")
    
    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    print()
    print_info("ç”Ÿæˆè¯¦ç»†ç›‘æ§æŠ¥å‘Š...")
    
    report = generate_monitoring_report(result)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = Path("è´¨é‡ç›‘æ§æŠ¥å‘Š.txt")
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print_success(f"ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    except Exception as e:
        print_error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    # è¿”å›çŠ¶æ€ç 
    if prediction['probability'] >= 0.7:
        sys.exit(0)  # æˆåŠŸ
    else:
        sys.exit(1)  # éœ€è¦æ”¹è¿›

if __name__ == "__main__":
    main()